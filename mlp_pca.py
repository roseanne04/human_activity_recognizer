# -*- coding: utf-8 -*-
"""mlp_pca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LlLNdijPvL5PNIUmcOtPYboDywsOdaKD
"""

import numpy as np
import pandas as pd

dataset=pd.read_csv('HAR.csv')

print(dataset.isna().sum())

dataset=dataset.to_numpy()

x=dataset[:,:-1]
y=dataset[:,-1]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)
#x_train,x_test,y_train,y_test=x[:5000],x[5000:],y[:5000],y[5000:]

#Applying PCA
from sklearn.decomposition import PCA
pca=PCA(0.95)
x_train_pca = pca.fit_transform(x_train)
x_test_pca = pca.transform(x_test)

from sklearn.linear_model import LogisticRegression
softmax_reg=LogisticRegression(multi_class="multinomial",solver="lbfgs",C=10)
softmax_reg.fit(x_train_pca,y_train)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
y_tt=softmax_reg.predict(x_train_pca)
print(accuracy_score(y_train,y_tt))
y_ttt=softmax_reg.predict(x_test_pca)
print(accuracy_score(y_test,y_ttt))
print(classification_report(y_test,y_ttt))
print(confusion_matrix(y_test,y_ttt))

ova=LogisticRegression(multi_class="ovr")
ova.fit(x_train_pca,y_train)
ova_pred=ova.predict(x_test_pca)
print(accuracy_score(y_test,ova_pred))
print(classification_report(y_test,ova_pred))
print(confusion_matrix(y_test,ova_pred))

from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
estimator = []
estimator.append(('LR',
                  LogisticRegression(solver ='lbfgs',
                                     multi_class ='multinomial',
                                     max_iter = 200)))
estimator.append(('SVC', SVC(gamma ='auto', probability = True)))
estimator.append(('DTC', DecisionTreeClassifier()))
# Voting Classifier with hard voting
vot_hard = VotingClassifier(estimators = estimator, voting ='hard')
vot_hard.fit(x_train_pca, y_train)
y_pred_hard = vot_hard.predict(x_test_pca)
y_pred_hard_train = vot_hard.predict(x_train_pca)
print(accuracy_score(y_test,y_pred_hard))
print(accuracy_score(y_train,y_pred_hard_train))
print(confusion_matrix(y_test,y_pred_hard))
print(classification_report(y_test,y_pred_hard))

def predicting_softmax_reg_pca(AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM,AN,AO,AP,AQ,AR,AS,AT,AU,AV,AW,AX,AY,AZ,BA,BB,BC,BD,BE,BF,BG,BH,BI,BJ,BK,BL,BM,BN,BO,BP,BQ,BR,BS,BT,BU,BV,BW,BX,BY,BZ,CA,CB,CC,CD,CE,CF,CG,CH,CI,CJ,CK,CL,CM,CN,CO,CP,CQ,CR,CS,CT,CU,CV,CW,CX,CY,CZ,DA,DB,DC,DD,DE,DF,DG,DH,DI,DJ,DK,DL,DM,DN,DO,DP,DQ,DR,DS,DT,DU,DV,DW,DX,DY,DZ,EA,EB,EC,ED,EE,EF,EG,EH,EI,EJ,EK,EL,EM,EN,EO,EP,EQ,ER,ES,ET,EU,EV,EW,EX,EY,EZ,FA,FB,FC,FD,FE,FF,FG,FH,FI,FJ,FK,FL,FM,FN,FO,FP,FQ,FR,FS,FT,FU,FV,FW,FX,FY,FZ,GA,GB,GC,GD,GE,GF,GG,GH,GI,GJ,GK,GL,GM,GN,GO,GP,GQ,GR,GS,GT,GU,GV,GW,GX,GY,GZ,HA,HB,HC,HD,HE,HF,HG,HH,HI,HJ,HK,HL,HM,HN,HO,HP,HQ,HR,HS,HT,HU,HV,HW,HX,HY,HZ,IA,IB,IC,ID,IE,IF,IG,IH,II,IJ,IK,IL,IM,IN,IO,IP,IQ,IR,IS,IT,IU,IV,IW,IX,IY,IZ,JA,JB,JC,JD,JE,JF,JG,JH,JI,JJ,JK,JL,JM,JN,JO,JP,JQ,JR,JS,JT,JU,JV,JW,JX,JY,JZ,KA,KB,KC,KD,KE,KF,KG,KH,KI,KJ,KK,KL,KM,KN,KO,KP,KQ,KR,KS,KT,KU,KV,KW,KX,KY,KZ,LA,LB,LC,LD,LE,LF,LG,LH,LI,LJ,LK,LL,LM,LN,LO,LP,LQ,LR,LS,LT,LU,LV,LW,LX,LY,LZ,MA,MB,MC,MD,ME,MF,MG,MH,MI,MJ,MK,ML,MM,MN,MO,MP,MQ,MR,MS,MT,MU,MV,MW,MX,MY,MZ,NA,NB,NC,ND,NE,NF,NG,NH,NI,NJ,NK,NL,NM,NN,NO,NP,NQ,NR,NS,NT,NU,NV,NW,NX,NY,NZ,OA,OB,OC,OD,OE,OF,OG,OH,OI,OJ,OK,OL,OM,ON,OO,OP,OQ,OR,OS,OT,OU,OV,OW,OX,OY,OZ,PA,PB,PC,PD,PE,PF,PG,PH,PI,PJ,PK,PL,PM,PN,PO,PP,PQ,PR,PS,PT,PU,PV,PW,PX,PY,PZ,QA,QB,QC,QD,QE,QF,QG,QH,QI,QJ,QK,QL,QM,QN,QO,QP,QQ,QR,QS,QT,QU,QV,QW,QX,QY,QZ,RA,RB,RC,RD,RE,RF,RG,RH,RI,RJ,RK,RL,RM,RN,RO,RP,RQ,RR,RS,RT,RU,RV,RW,RX,RY,RZ,SA,SB,SC,SD,SE,SF,SG,SH,SI,SJ,SK,SL,SM,SN,SO,SP,SQ,SR,SS,ST,SU,SV,SW,SX,SY,SZ,TA,TB,TC,TD,TE,TF,TG,TH,TI,TJ,TK,TL,TM,TN,TO,TP,TQ,TR,TS,TT,TU,TV,TW,TX,TY,TZ,UA,UB,UC,UD,UE,UF,UG,UH,UI,UJ,UK,UL,UM,UN,UO,UP,UQ,UR,US,UT,UU,UV,UW,UX,UY,UZ,VA,VB,VC,VD,VE,VF,VG,VH,VI,VJ,VK,VL,VM,VN,VO):
    pca_list=pca.transform([[AA,AB,AC,AD,AE,AF,AG,AH,AI,AJ,AK,AL,AM,AN,AO,AP,AQ,AR,AS,AT,AU,AV,AW,AX,AY,AZ,BA,BB,BC,BD,BE,BF,BG,BH,BI,BJ,BK,BL,BM,BN,BO,BP,BQ,BR,BS,BT,BU,BV,BW,BX,BY,BZ,CA,CB,CC,CD,CE,CF,CG,CH,CI,CJ,CK,CL,CM,CN,CO,CP,CQ,CR,CS,CT,CU,CV,CW,CX,CY,CZ,DA,DB,DC,DD,DE,DF,DG,DH,DI,DJ,DK,DL,DM,DN,DO,DP,DQ,DR,DS,DT,DU,DV,DW,DX,DY,DZ,EA,EB,EC,ED,EE,EF,EG,EH,EI,EJ,EK,EL,EM,EN,EO,EP,EQ,ER,ES,ET,EU,EV,EW,EX,EY,EZ,FA,FB,FC,FD,FE,FF,FG,FH,FI,FJ,FK,FL,FM,FN,FO,FP,FQ,FR,FS,FT,FU,FV,FW,FX,FY,FZ,GA,GB,GC,GD,GE,GF,GG,GH,GI,GJ,GK,GL,GM,GN,GO,GP,GQ,GR,GS,GT,GU,GV,GW,GX,GY,GZ,HA,HB,HC,HD,HE,HF,HG,HH,HI,HJ,HK,HL,HM,HN,HO,HP,HQ,HR,HS,HT,HU,HV,HW,HX,HY,HZ,IA,IB,IC,ID,IE,IF,IG,IH,II,IJ,IK,IL,IM,IN,IO,IP,IQ,IR,IS,IT,IU,IV,IW,IX,IY,IZ,JA,JB,JC,JD,JE,JF,JG,JH,JI,JJ,JK,JL,JM,JN,JO,JP,JQ,JR,JS,JT,JU,JV,JW,JX,JY,JZ,KA,KB,KC,KD,KE,KF,KG,KH,KI,KJ,KK,KL,KM,KN,KO,KP,KQ,KR,KS,KT,KU,KV,KW,KX,KY,KZ,LA,LB,LC,LD,LE,LF,LG,LH,LI,LJ,LK,LL,LM,LN,LO,LP,LQ,LR,LS,LT,LU,LV,LW,LX,LY,LZ,MA,MB,MC,MD,ME,MF,MG,MH,MI,MJ,MK,ML,MM,MN,MO,MP,MQ,MR,MS,MT,MU,MV,MW,MX,MY,MZ,NA,NB,NC,ND,NE,NF,NG,NH,NI,NJ,NK,NL,NM,NN,NO,NP,NQ,NR,NS,NT,NU,NV,NW,NX,NY,NZ,OA,OB,OC,OD,OE,OF,OG,OH,OI,OJ,OK,OL,OM,ON,OO,OP,OQ,OR,OS,OT,OU,OV,OW,OX,OY,OZ,PA,PB,PC,PD,PE,PF,PG,PH,PI,PJ,PK,PL,PM,PN,PO,PP,PQ,PR,PS,PT,PU,PV,PW,PX,PY,PZ,QA,QB,QC,QD,QE,QF,QG,QH,QI,QJ,QK,QL,QM,QN,QO,QP,QQ,QR,QS,QT,QU,QV,QW,QX,QY,QZ,RA,RB,RC,RD,RE,RF,RG,RH,RI,RJ,RK,RL,RM,RN,RO,RP,RQ,RR,RS,RT,RU,RV,RW,RX,RY,RZ,SA,SB,SC,SD,SE,SF,SG,SH,SI,SJ,SK,SL,SM,SN,SO,SP,SQ,SR,SS,ST,SU,SV,SW,SX,SY,SZ,TA,TB,TC,TD,TE,TF,TG,TH,TI,TJ,TK,TL,TM,TN,TO,TP,TQ,TR,TS,TT,TU,TV,TW,TX,TY,TZ,UA,UB,UC,UD,UE,UF,UG,UH,UI,UJ,UK,UL,UM,UN,UO,UP,UQ,UR,US,UT,UU,UV,UW,UX,UY,UZ,VA,VB,VC,VD,VE,VF,VG,VH,VI,VJ,VK,VL,VM,VN,VO]])
    num = softmax_reg.predict(pca_list)[0]
    di={1:"WALKING",
2:"WALKING_UPSTAIRS",
3:"WALKING_DOWNSTAIRS",
4:"SITTING",
5:"STANDING",
6:"LAYING"}
    return di[num]

print(predicting_softmax_reg_pca(0.27688,-0.012722,-0.103438,-0.994815,-0.973077,-0.985357,-0.995509,-0.973948,-0.985172,-0.940028,-0.554594,-0.81585,0.845442,0.684757,0.838455,-0.987883,-0.999965,-0.999724,-0.999667,-0.995868,-0.980265,-0.982755,-0.628997,-0.396748,-0.4239,0.345508,-0.173996,0.178123,0.169444,-0.020163,0.136034,-0.114406,0.243375,0.300912,-0.076445,0.081957,0.152679,0.008504,-0.355916,0.227309,0.969203,-0.152361,0.081258,-0.997265,-0.986488,-0.994693,-0.997386,-0.985851,-0.994744,0.894941,-0.172486,0.073599,0.987826,-0.126923,0.083048,-0.424134,0.914829,-0.965202,-0.988685,-0.997619,-0.984105,-0.995356,-1,-1,-1,-0.34725,0.384406,-0.422208,0.460732,-0.285763,0.238915,-0.238751,0.259128,-0.16737,0.21862,-0.270372,0.320243,0.723735,0.215377,0.821875,0.071397,0.016842,0.00103,-0.991234,-0.973264,-0.985018,-0.991622,-0.972164,-0.983356,-0.985976,-0.976007,-0.984936,0.991509,0.983485,0.98722,-0.98501,-0.999902,-0.999388,-0.999668,-0.987849,-0.973441,-0.981204,-0.753959,-0.576344,-0.661492,0.251356,-0.011399,0.355393,-0.026377,-0.118344,0.324212,-0.078466,0.320167,0.108127,0.053048,0.019118,0.222992,0.205179,0.256965,0.281472,-0.024708,-0.068885,0.085482,-0.984664,-0.988216,-0.980904,-0.985483,-0.989828,-0.983507,-0.871571,-0.948694,-0.747741,0.836739,0.907379,0.806705,-0.984499,-0.999861,-0.999896,-0.999754,-0.988762,-0.992667,-0.988931,-0.299374,-0.253805,-0.359201,-0.229788,0.120279,0.106303,0.171444,-0.185392,0.077084,0.062205,0.298176,-0.070191,0.081645,-0.052891,0.311698,-0.425716,0.15239,-0.478498,-0.099443,-0.032515,-0.045513,-0.985911,-0.993718,-0.984762,-0.98707,-0.994119,-0.987011,-0.984646,-0.99236,-0.979554,0.989222,0.994889,0.986899,-0.991327,-0.999845,-0.999965,-0.999797,-0.98981,-0.993453,-0.991377,-0.480607,-0.559265,-0.482153,-0.197139,0.009658,-0.065497,0.360868,-0.242841,0.042218,-0.004289,0.197173,-0.102798,0.086863,-0.04725,0.161177,-0.073904,0.196206,-0.436733,-0.98804,-0.985902,-0.98754,-0.987746,-0.992509,-0.98804,-0.999766,-0.989586,-0.61464,0.373113,-0.343033,0.344907,-0.27675,-0.98804,-0.985902,-0.98754,-0.987746,-0.992509,-0.98804,-0.999766,-0.989586,-0.61464,0.373113,-0.343033,0.344907,-0.27675,-0.984411,-0.986025,-0.987353,-0.978672,-0.980812,-0.984411,-0.999678,-0.991035,-0.720537,0.496394,-0.387692,-0.110102,-0.116492,-0.985251,-0.980248,-0.977188,-0.985717,-0.990011,-0.985251,-0.999788,-0.978947,-0.184288,-0.068882,-0.026641,0.138315,-0.142991,-0.991261,-0.990341,-0.990462,-0.992664,-0.993479,-0.991261,-0.99992,-0.989215,-0.471002,0.57177,-0.34927,-0.171443,-0.173459,-0.992886,-0.970608,-0.983672,-0.995811,-0.97511,-0.98683,-0.995049,-0.973767,-0.985131,-0.996466,-0.97917,-0.985181,-0.986483,-0.985579,-0.999575,-0.985001,-0.999964,-0.999371,-0.999652,-0.993555,-0.974271,-0.981006,-1,-0.812205,-0.837884,-0.548387,-1,-0.384615,0.254522,0.145806,0.356001,-0.448768,-0.809833,-0.42561,-0.737956,-0.623451,-0.800782,-0.999979,-0.999951,-0.999935,-0.999774,-0.999867,-0.999879,-0.999872,-0.999954,-0.999971,-0.999883,-0.999887,-0.999899,-0.99997,-0.999793,-0.999508,-0.999592,-0.999495,-0.999465,-0.999126,-0.999717,-0.999614,-0.999922,-0.99943,-0.999365,-0.999285,-0.999732,-0.999392,-0.999403,-0.999869,-0.999329,-0.999644,-0.99978,-0.99966,-0.999388,-0.999701,-0.999998,-0.99971,-0.999727,-0.999611,-0.999797,-0.999666,-0.999748,-0.990986,-0.973456,-0.981732,-0.992331,-0.974929,-0.987065,-0.990626,-0.97424,-0.985535,-0.993646,-0.979536,-0.989209,-0.989821,-0.994023,-0.995223,-0.982273,-0.999902,-0.999388,-0.999668,-0.992079,-0.977581,-0.982655,-1,-1,-1,0.12,-0.56,-0.68,0.298147,-0.116642,0.075871,-0.427557,-0.793565,-0.49097,-0.876289,-0.704126,-0.915115,-0.999976,-0.999944,-0.999932,-0.999783,-0.999887,-0.999886,-0.999786,-0.999986,-0.999955,-0.999861,-0.999891,-0.999787,-0.999942,-0.999778,-0.999681,-0.999683,-0.999428,-0.999367,-0.999082,-0.999619,-0.999656,-0.999985,-0.999644,-0.999293,-0.999138,-0.999701,-0.999491,-0.999292,-0.999842,-0.999439,-0.99967,-0.99976,-0.999672,-0.999369,-0.999464,-0.999947,-0.999497,-0.999744,-0.999566,-0.999486,-0.999592,-0.99971,-0.981913,-0.987823,-0.977272,-0.985469,-0.988464,-0.983949,-0.980543,-0.988035,-0.978263,-0.989082,-0.991691,-0.992534,-0.997837,-0.989716,-0.986205,-0.983396,-0.999853,-0.9999,-0.999732,-0.984652,-0.988368,-0.977084,-0.61755,-0.655244,-0.730537,-1,-0.548387,-0.724138,-0.279814,-0.051036,0.098829,-0.429648,-0.785393,-0.484831,-0.850799,-0.583694,-0.898025,-0.999875,-0.999805,-0.999943,-0.999947,-0.999901,-0.99993,-0.999975,-0.999998,-0.999855,-0.999935,-0.999904,-0.999985,-0.999854,-0.999935,-0.999883,-0.999946,-0.999992,-0.999993,-0.999977,-0.999922,-0.999878,-0.999885,-0.999887,-0.99999,-0.999966,-0.999865,-0.999892,-0.999985,-0.999812,-0.999803,-0.99986,-0.999961,-0.999806,-0.999816,-0.999726,-0.999825,-0.999757,-0.999851,-0.999814,-0.999769,-0.999741,-0.999917,-0.985697,-0.987189,-0.986765,-0.988628,-0.98558,-0.985697,-0.999758,-0.991582,-0.907014,-1,0.380843,-0.321316,-0.650227,-0.984763,-0.986617,-0.98349,-0.987666,-0.996215,-0.984763,-0.999745,-0.983666,-1,-1,0.501876,-0.516756,-0.785872,-0.983458,-0.981178,-0.981707,-0.97856,-0.986836,-0.983458,-0.999736,-0.980711,-0.640633,-1,-0.047159,0.007872,-0.28356,-0.991314,-0.989298,-0.99107,-0.987299,-0.980197,-0.991314,-0.999924,-0.991565,-0.923452,-1,0.120503,0.348771,0.057682,0.080699,0.595791,-0.475802,0.115931,-0.851562,0.187609,-0.034681))

import modelbit
ml=modelbit.login()

ml.deploy(predicting_softmax_reg_pca)

lms=[0.274262,-0.012265,-0.109494,-0.993865,-0.977421,-0.989336,-0.995502,-0.975182,-0.989586,-0.938716,-0.561801,-0.812655,0.840255,0.689579,0.843532,-0.98881,-0.999953,-0.999774,-0.999801,-0.99789,-0.974333,-0.989654,-0.669412,-0.38136,-0.579228,-0.027479,0.138216,-0.102128,0.157766,-0.041472,-0.042687,0.190033,-0.077577,0.298807,-0.13848,0.217925,-0.088719,-0.185076,0.002176,-0.235451,0.917237,0.119316,0.238581,-0.99553,-0.984233,-0.996702,-0.995318,-0.983378,-0.997055,0.843624,0.092834,0.229075,0.936834,0.141056,0.240431,-0.095234,0.779481,-0.963567,-0.892671,-0.994677,-0.98145,-0.997918,-1,-0.560427,-0.775959,-0.281789,0.292801,-0.30372,0.314654,-0.569477,0.53075,-0.519349,0.520359,-0.404778,0.430632,-0.456231,0.478964,-0.996856,0.63088,-0.669298,0.070491,0.012746,0.026885,-0.991798,-0.99161,-0.991944,-0.992585,-0.99168,-0.992442,-0.994611,-0.99067,-0.983417,0.986748,0.992092,0.995748,-0.994266,-0.999911,-0.999884,-0.999848,-0.993165,-0.991686,-0.992248,-0.74495,-0.84896,-0.761514,-0.015502,0.275806,0.13966,0.08133,-0.029208,-0.001424,0.129016,0.362534,0.181277,-0.070691,0.135828,0.149467,0.085967,0.239069,-0.048928,-0.030056,-0.090109,0.085033,-0.987364,-0.966599,-0.974426,-0.993125,-0.983659,-0.981409,-0.885939,-0.95302,-0.750639,0.823699,0.856668,0.776098,-0.986777,-0.999902,-0.999324,-0.999594,-0.998332,-0.995086,-0.986573,-0.700089,-0.606151,-0.364,0.000704,-0.167165,0.138978,0.399202,-0.360884,0.233229,-0.310537,0.587315,-0.250463,0.123383,-0.046416,0.273143,0.921745,0.771348,0.812544,-0.113281,-0.089832,-0.085486,-0.996012,-0.990604,-0.990742,-0.99656,-0.993237,-0.9938,-0.998368,-0.997679,-0.993222,0.991348,0.985748,0.983862,-0.995308,-0.999975,-0.999929,-0.999901,-0.996652,-0.996291,-0.996349,-0.863492,-0.762546,-0.723912,0.117043,-0.355106,-0.080722,0.194452,-0.250822,0.001814,-0.1395,-0.689748,-0.218008,-0.023735,-0.091999,-0.183671,0.449274,0.515015,0.634128,-0.989089,-0.992466,-0.992751,-0.986263,-0.992096,-0.989089,-0.99982,-0.991324,-0.732806,0.135963,-0.054403,-0.041758,-0.093604,-0.989089,-0.992466,-0.992751,-0.986263,-0.992096,-0.989089,-0.99982,-0.991324,-0.732806,0.135963,-0.054403,-0.041758,-0.093604,-0.993462,-0.992089,-0.992989,-0.988022,-0.985903,-0.993462,-0.999883,-0.993512,-0.824507,0.244323,-0.15546,0.056032,-0.271301,-0.987138,-0.946844,-0.970856,-0.93059,-0.993451,-0.987138,-0.999474,-0.993435,-0.452132,-0.430828,0.181621,0.006583,0.256734,-0.995195,-0.98748,-0.992098,-0.985965,-0.99211,-0.995195,-0.999938,-0.997339,-0.69472,0.189918,-0.013691,-0.18512,-0.221206,-0.991922,-0.984691,-0.984853,-0.99482,-0.974359,-0.992898,-0.992216,-0.982489,-0.993055,-0.997211,-0.970409,-0.988122,-0.995218,-0.998131,-0.985535,-0.989224,-0.999955,-0.999521,-0.999767,-0.987329,-0.991362,-0.989433,-1,-0.94487,-0.94419,-0.290323,-1,-0.923077,0.034262,-0.292158,0.818875,-0.641341,-0.944268,0.338743,0.124498,-0.505124,-0.710712,-0.999974,-0.999934,-0.999874,-0.999866,-0.999898,-0.999894,-0.999955,-0.999929,-0.999962,-0.999858,-0.999912,-0.999946,-0.999957,-0.999871,-0.999386,-0.999848,-0.999945,-0.999918,-0.999886,-0.999922,-0.999904,-0.999999,-0.999468,-0.99993,-0.999919,-0.999941,-0.999506,-0.999935,-0.999809,-0.999769,-0.999868,-0.999793,-0.999482,-0.998835,-0.998997,-0.998122,-0.999804,-0.999874,-0.999323,-0.998736,-0.999803,-0.999675,-0.992229,-0.990907,-0.989038,-0.991987,-0.993225,-0.993798,-0.990142,-0.992577,-0.992386,-0.992732,-0.994133,-0.991765,-0.98825,-0.981482,-0.960173,-0.991452,-0.99991,-0.999884,-0.999849,-0.989651,-0.99276,-0.991423,-1,-1,-1,-0.16,-0.56,-0.28,0.097447,-0.178677,0.074567,-0.251361,-0.710623,-0.589535,-0.938379,-0.588453,-0.870831,-0.999977,-0.999946,-0.999875,-0.99987,-0.999905,-0.999909,-0.999951,-0.999973,-0.999957,-0.999854,-0.999913,-0.999947,-0.999922,-0.999857,-0.999897,-0.999876,-0.999927,-0.999912,-0.999889,-0.999873,-0.999893,-0.999935,-0.99987,-0.999914,-0.999903,-0.999903,-0.999875,-0.999929,-0.999558,-0.999743,-0.99988,-0.999917,-0.999802,-0.999513,-0.999571,-0.99952,-0.999657,-0.999927,-0.999709,-0.999539,-0.999807,-0.999861,-0.980991,-0.957752,-0.965357,-0.989504,-0.973642,-0.980557,-0.985302,-0.973492,-0.977293,-0.992347,-0.981175,-0.980385,-0.981975,-0.866201,-0.929924,-0.966391,-0.999896,-0.999356,-0.999559,-0.986015,-0.983372,-0.973374,-0.648162,-0.332157,-0.598829,-0.866667,-1,-1,0.155467,0.371322,0.449739,-0.518869,-0.849138,-0.497792,-0.872246,-0.047512,-0.322069,-0.99991,-0.999908,-0.999924,-0.999935,-0.999783,-0.999843,-0.999807,-0.999729,-0.999902,-0.999914,-0.999787,-0.999773,-0.9999,-0.999893,-0.999281,-0.999772,-0.999896,-0.999842,-0.999573,-0.998905,-0.997215,-0.996046,-0.999368,-0.999856,-0.999432,-0.99633,-0.999364,-0.999702,-0.999676,-0.999784,-0.999776,-0.999814,-0.999553,-0.999242,-0.998541,-0.998296,-0.999621,-0.999699,-0.999474,-0.998435,-0.999595,-0.999711,-0.993633,-0.991827,-0.990921,-0.993392,-0.995153,-0.993633,-0.999895,-0.991463,-0.946182,-0.931034,0.170336,-0.442209,-0.745004,-0.991841,-0.99094,-0.989986,-0.990749,-0.988171,-0.991841,-0.999879,-0.991104,-1,-0.936508,0.17882,-0.370944,-0.693222,-0.941265,-0.961012,-0.951783,-0.975094,-0.841382,-0.941265,-0.998404,-0.959432,-0.368074,-0.897436,0.215672,-0.630511,-0.922003,-0.989085,-0.985876,-0.983975,-0.99089,-0.991007,-0.989085,-0.999884,-0.987774,-0.809976,-0.936508,-0.478401,-0.408331,-0.836165,0.103368,0.054563,0.303959,0.306739,-0.734018,0.001672,-0.147169]
modelbit.get_inference(
  region="us-east-2.aws",
  workspace="sathwikbc",
  deployment="predicting_softmax_reg_pca",
  data=lms
)

lms=[0.27783,-0.017684,-0.105704,-0.996143,-0.995969,-0.994889,-0.996375,-0.995646,-0.993974,-0.939881,-0.575722,-0.822432,0.850248,0.694034,0.84748,-0.997367,-0.999976,-0.999981,-0.999912,-0.995911,-0.996069,-0.991375,-0.627971,-0.786144,-0.553377,0.392568,-0.185941,0.217984,0.364976,0.251511,-0.083042,0.103854,0.17916,0.474851,-0.171299,0.119852,0.003722,-0.207044,0.148761,-0.24748,-0.207026,0.769628,0.592192,-0.998868,-0.997782,-0.99647,-0.998892,-0.997843,-0.99652,-0.275726,0.722194,0.57982,-0.161663,0.78419,0.593851,0.453748,-0.934196,0.196987,-0.319542,-0.998893,-0.99797,-0.996526,-1,-0.816017,-1,-0.203677,0.265724,-0.328933,0.393323,-0.292446,0.254737,-0.263854,0.293196,-0.171895,0.200949,-0.230232,0.257515,-0.928786,0.873694,-0.720809,0.074493,0.015422,0.004917,-0.993315,-0.994463,-0.992784,-0.992606,-0.993243,-0.991169,-0.995663,-0.996844,-0.995835,0.988845,0.998297,0.991865,-0.994209,-0.999932,-0.999931,-0.999868,-0.988292,-0.992941,-0.986849,-0.752955,-0.858956,-0.774686,0.188532,0.043464,0.17708,0.438031,0.116759,0.179468,0.049019,0.578885,0.332369,0.072925,0.163949,0.162267,-0.166441,0.188578,-0.346805,-0.02819,-0.075922,0.085242,-0.998436,-0.994914,-0.994379,-0.99837,-0.994677,-0.994203,-0.886518,-0.955276,-0.756927,0.845137,0.913295,0.826587,-0.99625,-0.999997,-0.999974,-0.999962,-0.998033,-0.994066,-0.994272,-0.737541,-0.549335,-0.573314,0.387683,-0.212179,0.344614,0.080085,0.137234,-0.222272,0.202227,0.176635,0.246118,-0.173869,0.159101,-0.090725,0.24569,0.173923,0.556186,-0.098787,-0.043011,-0.055591,-0.9974,-0.998239,-0.996672,-0.997491,-0.998159,-0.996614,-0.994419,-0.99848,-0.994688,0.995763,0.998544,0.997344,-0.99841,-0.999986,-0.999994,-0.999975,-0.996876,-0.998068,-0.995836,-0.853855,-0.827417,-0.725647,0.363892,-0.082333,0.335724,0.442197,0.151729,-0.060068,0.098364,0.441884,0.391057,-0.065372,0.365259,-0.008709,0.339671,-0.286002,-0.104936,-0.997867,-0.998682,-0.998572,-0.996559,-0.996814,-0.997867,-0.999979,-0.997367,-0.944055,0.517299,-0.373905,0.317723,-0.422069,-0.997867,-0.998682,-0.998572,-0.996559,-0.996814,-0.997867,-0.999979,-0.997367,-0.944055,0.517299,-0.373905,0.317723,-0.422069,-0.994078,-0.9966,-0.996353,-0.996907,-0.992464,-0.994078,-0.999914,-0.995342,-0.908344,0.221348,0.005118,-0.170056,-0.249267,-0.99624,-0.998006,-0.997704,-0.998064,-0.990588,-0.99624,-0.999979,-0.998182,-0.764485,0.487509,-0.552766,0.399416,-0.22271,-0.998539,-0.998627,-0.998918,-0.997481,-0.990866,-0.998539,-0.999993,-0.999304,-0.843877,0.756545,-0.573744,-0.093778,-0.104037,-0.99518,-0.993882,-0.993607,-0.996581,-0.996422,-0.995356,-0.995719,-0.995375,-0.994292,-0.997466,-0.997084,-0.993234,-0.994002,-0.996327,-0.998206,-0.99667,-0.999976,-0.999951,-0.999903,-0.993019,-0.991902,-0.98907,-1,-1,-0.94419,-0.419355,-0.8,-0.692308,0.192916,0.343348,0.584649,-0.501437,-0.854469,-0.644821,-0.906513,-0.665402,-0.838026,-0.999998,-0.999931,-0.999914,-0.999922,-0.999956,-0.999835,-0.999978,-0.99999,-0.999981,-0.999909,-0.999926,-0.999982,-0.999978,-0.999918,-0.999944,-0.999959,-0.999943,-0.999931,-0.999787,-0.999856,-0.999922,-0.999942,-0.999953,-0.999932,-0.999822,-0.999928,-0.999948,-0.999915,-0.999901,-0.999893,-0.999863,-0.999784,-0.999856,-0.999604,-0.99972,-0.999846,-0.999917,-0.999868,-0.999826,-0.999764,-0.999912,-0.999812,-0.99308,-0.994271,-0.990698,-0.994212,-0.995124,-0.993575,-0.99206,-0.994019,-0.992724,-0.996772,-0.996472,-0.99323,-0.996043,-0.996853,-0.976843,-0.993604,-0.999932,-0.999931,-0.999868,-0.98905,-0.993633,-0.986641,-1,-1,-1,-0.36,0.16,0.16,0.239107,0.062977,0.266898,-0.678268,-0.930351,-0.69757,-0.982042,-0.674942,-0.915274,-0.999998,-0.999936,-0.999927,-0.999918,-0.999949,-0.99978,-0.999965,-0.999988,-0.999959,-0.999916,-0.999889,-0.999963,-0.999944,-0.999877,-0.999951,-0.999956,-0.999926,-0.999941,-0.999853,-0.999824,-0.999971,-0.999975,-0.999955,-0.999927,-0.999853,-0.999976,-0.999937,-0.999928,-0.999874,-0.999868,-0.999879,-0.999789,-0.999836,-0.99963,-0.999555,-0.999832,-0.999876,-0.999864,-0.999771,-0.999559,-0.999903,-0.999808,-0.997498,-0.995845,-0.995083,-0.998767,-0.994285,-0.994491,-0.998366,-0.995967,-0.995411,-0.998728,-0.992904,-0.993074,-0.999501,-0.995097,-0.993382,-0.997086,-0.999996,-0.999974,-0.999959,-0.997242,-0.99671,-0.995156,-1,-0.910742,-0.952734,-1,-1,-1,0.332185,-0.124006,0.216569,-0.635927,-0.857105,0.14542,-0.148763,0.228257,0.006673,-0.999998,-0.999984,-0.999988,-0.999994,-0.999974,-0.99997,-0.999986,-1,-0.999996,-0.99999,-0.999971,-0.999992,-0.999996,-0.999988,-0.999961,-0.999993,-0.999997,-0.999999,-0.999992,-0.999975,-0.999983,-0.999989,-0.99997,-0.999997,-0.999989,-0.999983,-0.999971,-0.999996,-0.999965,-0.999978,-0.999976,-0.999987,-0.999968,-0.99998,-0.99994,-0.999973,-0.999961,-0.999974,-0.999977,-0.999955,-0.99996,-0.999986,-0.998113,-0.998652,-0.998044,-0.998946,-0.998879,-0.998113,-0.999985,-0.997097,-1,-0.37931,0.765261,-0.834582,-0.953077,-0.995831,-0.996734,-0.99619,-0.995096,-0.983274,-0.995831,-0.999956,-0.995585,-1,-0.777778,0.438074,-0.546191,-0.759096,-0.998203,-0.998045,-0.998039,-0.998397,-0.995301,-0.998203,-0.999991,-0.998484,-1,-1,0.320742,-0.602774,-0.875093,-0.998489,-0.998534,-0.998291,-0.998964,-0.998677,-0.998489,-0.999995,-0.997558,-1,-0.968254,0.564689,-0.716752,-0.930401,-0.196544,-0.665017,0.772096,0.775334,0.397758,-0.544442,-0.433302]

import requests

# The API endpoint
url = "https://sathwikbc.us-east-2.aws.modelbit.com/v1/predicting_softmax_reg_pca/latest"

# Data to be sent
data = {
    "data":lms
}

# A POST request to the API
response = requests.post(url, json=data)

# Print the response
print(response.json())



